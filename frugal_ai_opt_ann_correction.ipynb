{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855cdd86"
      },
      "source": [
        "# **Frugal AI: Rethinking model design and objective for a sustainable future**\n",
        "\n",
        "**PSL-Week: Practical work notebook for part 2**\n",
        "\n",
        "This document presents instructions regarding the second practical work session of the Frugal-AI PSL-Week. All the materials (slides, notebooks, base codes) can be recovered from the corresponding [GitHub repository](https://github.com/Deyht/frugal_ai).\n",
        "\n",
        "The notebook is expected to be run on a Google Colab environment with a T4 GPU:\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deyht/frugal_ai/blob/main/frugal_ai_opt_ann_correction.ipynb)\n",
        "\n",
        "\n",
        "This notebook is **not intended as a standalone** document; the slides should be used as a reference to understand several of the explanations provided in the analysis of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Energy budget based CNN base optimization on GPU\n",
        "\n",
        "This part addresses the optimization of Artificial Neural Network models for numerical efficiency and, therefore, energy efficiency.\n",
        "\n",
        "For this exercise, we will use the ASIRRA (Animal Species Image Recognition for Restricting Access) dataset, which comprises 25000 labeled images of Cats and Dogs. The objective is to explore network structures, following the lecture's guidelines, to build a classification model that **maximizes classification accuracy under a given energy budget**. As discussed in the slides, both training and inference contribute to the model's environmental budget. To simplify the problem so it is compatible with a practical work session, we will assume the model's training cost is negligible and account only for electricity consumption during inference (which would match a long-lasting, broadly distributed model). We will also not factor in any infrastructure cost. Those are very strong approximations, and we invite the participants to refer back to the slide for a more complete view of the impact of ITC systems. At the end, for our practical case, we consider that the energy consumed is simply proportional to the execution time.\n",
        "\n",
        "This practical work will be carried out through Google Colab, granting each participant access to an Nvidia T4 GPU (Turing, 2560 CUDA cores, 16 GB RAM, peak FP32: 8.141 TFLOPS, and FP16: 65.13 TFLOPS, 70W peak consumption). Our energy budget will be based on the compute time required to process a set of 2048 test images, with a maximum budget of 300 ms. Your objective will be to design a model that achieves the highest possible classification accuracy (maximizing usefulness) while remaining under this budget. Note that a model reaching only 80% accuracy on this task (1 error out of 5 predictions) can barely be considered useful.\n",
        "\n",
        "This notebook handled the data loading, model training, and inference based on a simple LeNET-5-like architecture in opt_cnn/. The ANN model is built and trained using the custom [CIANNA](https://github.com/Deyht/CIANNA) framework. You should be able to adapt the architecture with only minor adjustments to the appropriate notebook cells, taking inspiration from the provided example. You can refer to CIANNA's WIKI page for a complete framework description and access to the full list of supported layers and their configurations. You can also look at some example scripts provided in CIANNA's GitHub repository. Of course, you are also free to replace these parts with any other Deep Learning development framework you are familiar with (TensorFlow, PyTorch, etc.), as long as you correctly measure the inference time using a T4 GPU over Google Colab."
      ],
      "metadata": {
        "id": "n_y-z2dR48xO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CIANNA installation**\n",
        "\n",
        "This notebook uses CIANNA to build and train network architectures. CIANNA is a general-purpose artificial neural network development framework (like TensorFlow or PyTorch) designed mainly for astronomical applications. Still, it is perfectly usable for classical CNN implementations. It is very fast for small networks, and it provides several performance measurement tools that will be useful in the context of this exercise.\n",
        "\n",
        "CIANNA is coded in C and CUDA for GPU acceleration and is controlled through a Python interface. As it is not available in Colab by default, we start by installing it.\n",
        "\n",
        "**Link to the CIANNA github repository**\n",
        "https://github.com/Deyht/CIANNA"
      ],
      "metadata": {
        "id": "vIXMFIFmvYzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query GPU allocation and properties\n",
        "\n",
        "If nvidia-smi fails, it might indicate that you launched the colab session without GPU reservation.  \n",
        "To change the type of reservation go to \"Runtime\"->\"Change runtime type\" and select \"GPU\" as your hardware accelerator."
      ],
      "metadata": {
        "id": "Ke8s2bCZvk1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi\n",
        "\n",
        "cd /content/\n",
        "\n",
        "git clone https://github.com/NVIDIA/cuda-samples/\n",
        "\n",
        "cd /content/cuda-samples/Samples/1_Utilities/deviceQuery/\n",
        "\n",
        "cmake CMakeLists.txt\n",
        "\n",
        "make SMS=\"50 60 70 80\"\n",
        "\n",
        "./deviceQuery | grep Capability | cut -c50- > ~/cuda_infos.txt\n",
        "./deviceQuery | grep \"CUDA Driver Version / Runtime Version\" | cut -c57- >> ~/cuda_infos.txt\n",
        "\n",
        "cd ~/\n"
      ],
      "metadata": {
        "id": "AHq06Uwk49Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clone CIANNA git repository"
      ],
      "metadata": {
        "id": "A1SJ6-x8vqsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compiling CIANNA for the allocated GPU generation\n",
        "\n",
        "There is no guaranteed forward or backward compatibility between Nvidia GPU generations, and some capabilities are generation-specific. For these reasons, CIANNA must be provided the platform GPU generation at compile time.\n",
        "The following cell will automatically update all the necessary files based on the detected GPU and compile CIANNA."
      ],
      "metadata": {
        "id": "JYGPC3OUv0td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "git clone https://github.com/Deyht/CIANNA\n",
        "\n",
        "cd CIANNA\n",
        "\n",
        "mult=\"10\"\n",
        "cat ~/cuda_infos.txt\n",
        "comp_cap=\"$(sed '1!d' ~/cuda_infos.txt)\"\n",
        "cuda_vers=\"$(sed '2!d' ~/cuda_infos.txt)\"\n",
        "\n",
        "lim=\"11.1\"\n",
        "old_arg=$(awk '{if ($1 < $2) print \"-D CUDA_OLD\";}' <<<\"${cuda_vers} ${lim}\")\n",
        "\n",
        "sm_val=$(awk '{print $1*$2}' <<<\"${mult} ${comp_cap}\")\n",
        "\n",
        "gen_val=$(awk '{if ($1 >= 80) print \"-D GEN_AMPERE\"; else if($1 >= 70) print \"-D GEN_VOLTA\";}' <<<\"${sm_val}\")\n",
        "\n",
        "sed -i \"s/.*arch=sm.*/\\\\t\\tcuda_arg=\\\"\\$cuda_arg -D CUDA -D comp_CUDA -lcublas -lcudart -arch=sm_$sm_val $old_arg $gen_val\\\"/g\" compile.cp\n",
        "sed -i \"s/\\/cuda-[0-9][0-9].[0-9]/\\/cuda-$cuda_vers/g\" compile.cp\n",
        "sed -i \"s/\\/cuda-[0-9][0-9].[0-9]/\\/cuda-$cuda_vers/g\" src/python_module_setup.py\n",
        "\n",
        "./compile.cp CUDA PY_INTERF\n",
        "\n",
        "mv src/build/lib.linux-x86_64-* src/build/lib.linux-x86_64"
      ],
      "metadata": {
        "id": "HGJUvmWW7YE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing CIANNA installation\n",
        "\n",
        "**IMPORTANT NOTE**   \n",
        "CIANNA is mainly used in a script fashion and was not designed to run in notebooks. Every cell code that directly invokes CIANNA functions must be run as a script to avoid possible errors.  \n",
        "To do so, the cell must have the following structure:\n",
        "\n",
        "```\n",
        "%%shell\n",
        "\n",
        "cd /content/CIANNA\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "[... your python code ...]\n",
        "\n",
        "EOF\n",
        "```\n",
        "\n",
        "This syntax allows one to easily edit python code in the notebook while running the cell as a script. Note that all the notebook variables can not be accessed by the cell in this context, and all variables declared in the cell only exist there.\n",
        "\n",
        "Another drawback of this solution is that it will copy the cell content in the output in case of error. If such a cell does not run, scroll back to the beginning of the cell output to get the actual error message.\n"
      ],
      "metadata": {
        "id": "vbnBhbIL8wv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ASIRRA**\n",
        "\n",
        "The ASIRRA (Animal Species Image Recognition for Restricting Access) is a dataset that was originally used for CAPTCHA and HIP (Human Interactive Proofs).\n",
        "\n",
        "The original dataset comprises 25000 images of variable resolution (averaging around 350x500) and is equally distributed over the two classes \"Cat\" and \"Dog\". For this exercise, we provide two reduced versions in the form of padded and resized RGB images at either 128x128 or 256x256 as two binary files. This construction is necessary so the dataset can fit into the limited amount of Colab RAM. You can download one or both sets to test the impact of the input resolution on your network. In these files, the first 12500 images are of Cats, and the next 12500 are of Dogs. The last 1024 images of each class will be excluded to form our reference test dataset (total size 2048)."
      ],
      "metadata": {
        "id": "gd2waB3JYNkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading and visualizing the data\n",
        "\n",
        "We start by downloading and visualizing the raw data. You can get one or both of the resized versions."
      ],
      "metadata": {
        "id": "kULtlVy8Y5UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content\n",
        "\n",
        "wget https://share.obspm.fr/s/6TBsCpAASeETH3S/download/asirra_bin_128.tar.gz\n",
        "tar -xvzf asirra_bin_128.tar.gz\n",
        "\n",
        "#wget https://share.obspm.fr/s/52nxyfn7PjzawSe/download/asirra_bin_256.tar.gz\n",
        "#tar -xvzf asirra_bin_256.tar.gz"
      ],
      "metadata": {
        "id": "JvmqIq51H3-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "image_size = 128\n",
        "\n",
        "v_width = 8; v_height = 5\n",
        "nb_images = v_width*v_height\n",
        "\n",
        "f_im_s = image_size*image_size*3\n",
        "\n",
        "subset_cats = np.reshape(np.fromfile(\"asirra_bin_%d.dat\"%(image_size),\n",
        "  dtype=\"uint8\", count=f_im_s*(nb_images//2)), (nb_images//2,image_size,image_size,3))\n",
        "\n",
        "subset_dogs = np.reshape(np.fromfile(\"asirra_bin_%d.dat\"%(image_size),\n",
        "  dtype=\"uint8\", count=f_im_s*(nb_images//2), offset=12500*f_im_s), (nb_images//2,image_size,image_size,3))\n",
        "\n",
        "fig, ax = plt.subplots(v_height, v_width, figsize=(v_width*1.5,v_height*1.5), dpi=200, constrained_layout=True)\n",
        "\n",
        "for i in range(0, v_width*v_height):\n",
        "  c_x = i // v_width; c_y = i % v_width\n",
        "  p_c = int((i)%2) #Alternate cats and dogs in display\n",
        "  if(p_c == 0):\n",
        "    ax[c_x,c_y].imshow(subset_cats[i//2])\n",
        "  else:\n",
        "    ax[c_x,c_y].imshow(subset_dogs[i//2])\n",
        "  ax[c_x,c_y].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mjcrByRgYof3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data handling and augmentation\n",
        "\n",
        "To ease data manipulation and hyperparameter exploration, we first provide a set of helper functions. To make them accessible within the CIANNA script cells, we need to export them to a Python file. Every time you want to change the content of these functions, you will need to rerun the cell to generate a new .py file. If loaded in an interactive cell, you will need to restart the kernel after changing this file to re-import it properly."
      ],
      "metadata": {
        "id": "6ZeV1bvjRS4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, sys, gc, glob, time, cv2\n",
        "from threading import Thread\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "\n",
        "def data_prep(nb_images_per_iter, raw_image_size, image_size, test_mode=0):\n",
        "  #Data arrays are declared as global so we can work in place to reduce RAM footpring\n",
        "  global raw_images, input_data, targets, input_val, targets_val\n",
        "\n",
        "  raw_images = np.reshape(np.fromfile(\"asirra_bin_%d.dat\"%(raw_image_size), dtype=\"uint8\"), (25000, raw_image_size, raw_image_size,3))\n",
        "\n",
        "  if(test_mode == 0):\n",
        "    input_data = np.zeros((nb_images_per_iter,3*image_size**2), dtype=\"float32\") #CIANNA expects \"float32\" arrays\n",
        "    targets = np.zeros((nb_images_per_iter,2), dtype=\"float32\")\n",
        "\n",
        "  input_val = np.zeros((2048,3*image_size**2), dtype=\"float32\")\n",
        "  targets_val = np.zeros((2048,2), dtype=\"float32\")\n",
        "\n",
        "\n",
        "def create_augmented_batch(A_transform):\n",
        "\n",
        "  nb_images = np.shape(input_data)[0]\n",
        "\n",
        "  for i in range(0,nb_images):\n",
        "\n",
        "    l_class = np.random.randint(0,2)\n",
        "    l_id = np.random.randint(0,12500 - 1024)\n",
        "    #Last 1024 iamges of each class kept only for the val/test set\n",
        "\n",
        "    patch = raw_images[l_class*12500+l_id]\n",
        "    transformed = A_transform(image=patch)\n",
        "    patch_aug = transformed['image']\n",
        "\n",
        "    image_size = np.shape(patch_aug)[0]\n",
        "\n",
        "    #CIANNA expects data formated as 2D numpy arrays representing a list of flattened images (with every channel flattened after the others)\n",
        "    for depth in range(0,3): #We normalize based on mean pixel value\n",
        "      input_data[i,depth*image_size**2:(depth+1)*image_size**2] = (patch_aug[:,:,depth].flatten(\"C\") - 100.0)/155.0\n",
        "\n",
        "    targets[i,:] = 0.0\n",
        "    targets[i,l_class] = 1.0\n",
        "\n",
        "  return input_data, targets\n",
        "\n",
        "\n",
        "def create_validation_set(A_transform):\n",
        "\n",
        "  for i in range(0,2048):\n",
        "\n",
        "    l_class = i // 1024\n",
        "\n",
        "    patch = raw_images[(1+l_class)*(12500 - 1024) + i]\n",
        "    transformed = A_transform(image=patch)\n",
        "    patch_aug = transformed['image']\n",
        "\n",
        "    image_size = np.shape(patch_aug)[0]\n",
        "\n",
        "    for depth in range(0,3):\n",
        "      input_val[i,depth*image_size**2:(depth+1)*image_size**2] = (patch_aug[:,:,depth].flatten(\"C\") - 100.0)/155.0\n",
        "\n",
        "    targets_val[i,:] = 0.0\n",
        "    targets_val[i,l_class] = 1.0\n",
        "\n",
        "  return input_val, targets_val\n",
        "\n",
        "\n",
        "def free_data_helper():\n",
        "  global raw_images, input_data, targets, input_val, targets_val\n",
        "  del (raw_images, input_data, targets, input_val, targets_val)\n",
        "  return"
      ],
      "metadata": {
        "id": "HOKdPWXQLziA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test the helper functions with a simple example and display the produced images. The next cell illustrates how we can create an augmented batch of images for training from the raw image dataset.  \n",
        "\n",
        "Use this example to test the effect of combining different transform operations for data augmentation. You can also test the impact of the image resolution and of the position of the resize transformation in the augmentation list."
      ],
      "metadata": {
        "id": "bPo_XyxkXTy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper import *\n",
        "\n",
        "class_text = [\"cat\",\"dog\"]\n",
        "raw_image_size = 128\n",
        "image_size = 128\n",
        "\n",
        "v_width = 8; v_height = 5\n",
        "nb_images_per_iter = v_width*v_height\n",
        "\n",
        "#See Albumentation documentation for a list of existing augmentations\n",
        "train_transform = A.Compose([\n",
        "  #Image resize can be done after all other transform to preserve as much details as possible\n",
        "  #or as the fist operation so other transforms are faster\n",
        "  A.Resize(image_size,image_size, interpolation=2, p=1.0),\n",
        "  A.HorizontalFlip(p=0.5),\n",
        "  #Affine here act more as an aspect ratio transform than a scaling variation\n",
        "  A.Affine(scale=(0.3,1.3), translate_percent=(-0.3,0.3), rotate=(-10,10), interpolation=2, p=1.0),\n",
        "  A.ToGray(p=0.02),\n",
        "  A.ColorJitter(brightness=(0.8,1.2), contrast=(0.8,1.2), saturation=(0.8,1.2), hue=0.15, p=1.0),\n",
        "  ])\n",
        "\n",
        "val_transform = A.Compose([ #Here only a resize, but val transform could be more complex (center crop, padding, etc)\n",
        "  A.Resize(image_size, image_size, interpolation=2, p=1.0)])\n",
        "\n",
        "data_prep(nb_images_per_iter, raw_image_size, image_size)\n",
        "input_data, targets = create_augmented_batch(train_transform)\n",
        "\n",
        "fig, ax = plt.subplots(v_height, v_width, figsize=(v_width*1.5,v_height*1.5), dpi=200, constrained_layout=True)\n",
        "patch = np.zeros((image_size,image_size,3), dtype=\"uint8\")\n",
        "\n",
        "for i in range(0, v_width*v_height):\n",
        "  c_x = i // v_width; c_y = i % v_width\n",
        "  #Images in the augmented input_data array are directly in the CIANNA format.\n",
        "  #We need to convert them back to classical RGB for display.\n",
        "  for depth in range(0,3):\n",
        "    patch[:,:,depth] = np.reshape(input_data[i,depth*image_size**2:(depth+1)*image_size**2]*155 + 100,(image_size,image_size))\n",
        "\n",
        "  ax[c_x,c_y].imshow(patch)\n",
        "  ax[c_x,c_y].text(4, 10, class_text[(np.argmax(targets[i]))], c=\"red\", fontsize=10, clip_on=True)\n",
        "  ax[c_x,c_y].axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "free_data_helper()"
      ],
      "metadata": {
        "id": "yZYLSyJWXQ8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training a network\n",
        "\n",
        "The following cell trains a very simple LeNET-5-inspired network on the ASIRRA dataset at a low resolution of 32x32. This network will be very fast at training and inference, but it can only reach a low classification accuracy.\n",
        "\n",
        "As stated at the beginning of the notebook, **your objective is to find an architecture that maximizes classification accuracy, while keeping the inference time under 300 ms**.\n",
        "\n",
        "The only limiting rules are not to use external data or pretrained models on another dataset, not to change the data split between training and validation/test, and to run inference on a T4 GPU on Colab to measure inference time.\n",
        "Other than that, you are free to:\n",
        " * Change the input image resolution\n",
        " * Modify/expand the augmentation policy\n",
        " * Modify/expand the network backbone\n",
        "\n",
        "To stimulate your search, we provide a shared [Google Sheet](https://docs.google.com/spreadsheets/d/1l3l5zfHfAsi5AF4IU4P-5OQs0nl4cRzQpoHdWkRBqUc/edit?usp=sharing) to be used as a dynamic leaderboard. You can tackle this part individually, in pairs, or by forming teams. Once you have a working model, you can enter its properties/results in the leaderboard. It will only be accessible for the duration of this course.\n",
        "\n",
        "**Some strategies to help you explore model architectures:**\n",
        " * 1) Try to add layers in the network backbone and evaluate their impact on accuracy and compute time.\n",
        " * 2) Use the provided table of performance per layer to identify bottlenecks in your architecture (Remember that a layer's performance is not only dictated by its own parameters but also by the shape of its input, which depends on the configuration of the previous layer).\n",
        " * 3) When increasing the number of parameters in your architecture (number of layers or their size), you will be more prone to overtraining (test set error increasing). For this reason, changes to the architecture should be accompanied by changes to your image augmentation policy. Overtraining can be monitored by following the \"fwd mean loss\" reported by CIANNA, which is computed on the validation dataset (identical to the test dataset in our case). The validation error as a function of the epoch is also reported in the *error.txt* that is created automatically by CIANNA.\n",
        " * 4) Increasing the image size is a good way to improve accuracy, but there is a diminishing return at some point. However, this will have a strong negative impact on your computing time. Try to reduce the activation maps' resolution more aggressively over a few layers when using higher input resolutions. Keep an eye on the receptive field of the convolutional part of your backbone.\n",
        "\n",
        "**Points to keep an eye on:**\n",
        " * The size of the last activation maps before your first dense layer will have a strong impact on the number of parameters in your model, but it will not necessarily correlate with its accuracy. Keep an eye on the size of your activation maps before the first dense layer. You could also try to build a fully convolutional architecture to get rid of dense layers. This also has the advantage of making your trained model \"compatible\" with many input image resolutions. Again, the receptive field will be very important with such architecture. Examples are available in the CIANNA repository (see the ImageNET network backbone).\n",
        " * The batch size has a direct effect on the compute speed of your model, with larger values allowing faster computing (up to some limit). However, you are limited by the amount of GPU memory used by your model. Also, in this exercise, speed only matters at inference time. In contrast, small batch sizes are usually preferable during training to achieve gigher accuracy and converge in fewer training steps.\n",
        " * Using mixed precision is also an efficient way to increase computing speed, but it can reduce the accuracy (usually by only a small amount). Unless you choose to use a very deep architecture, you will likely achieve better results with FP16C_FP32A activated for both training and inference.\n",
        " * Finally, use existing knowledge available regarding architecture design. You are free to be inspired by any architecture you know or find. You will find architecture examples in the CIANNA repository, but you are free to explore various architecture designs (AlexNET, VGG, DenseNET, YOLO/darknet, etc), as long as the required layers are available in CIANNA.\n",
        "\n",
        "*Link to the [CIANNA](https://github.com/Deyht/CIANNA) repository. You can refer to CIANNA's [WIKI page](https://github.com/Deyht/CIANNA/wiki) for a complete framework description. You can also look at the full [API documentation](https://github.com/Deyht/CIANNA/wiki/4\\)-Interface-API-documentation) to add layer types that are absent from the LeNET-5 example.\n",
        "The saved models are available in the \"net_save\" repository that is automatically created when starting network training. The default naming scheme only refers to the training iteration, so rename your saving files with comprehensive information about your model to keep track of your progress. A saved model can be uploaded to a new Colab session for inference or further training.*"
      ],
      "metadata": {
        "id": "so4ypoJhLFkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd /content/\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "from helper import *\n",
        "\n",
        "sys.path.insert(0,glob.glob('/content/CIANNA/src/build/lib.*/')[-1])\n",
        "import CIANNA as cnn\n",
        "\n",
        "def i_ar(int_list):\n",
        "  return np.array(int_list, dtype=\"int\")\n",
        "\n",
        "raw_image_size = 128\n",
        "image_size = 128\n",
        "nb_images_per_iter = 4096 #Must likely be reduced if the image size is aumgented so examples can fit in RAM\n",
        "\n",
        "\n",
        "#See Albumentation documentation for a list of existing augmentations\n",
        "train_transform = A.Compose([\n",
        "  #Image resize can be done after all other transform to preserve as much details as possible\n",
        "  #or as the fist operation so other transforms are faster\n",
        "  A.Resize(image_size,image_size, interpolation=2, p=1.0),\n",
        "  A.HorizontalFlip(p=0.5),\n",
        "  #Affine here act more as an aspect ratio transform than a scaling variation\n",
        "  A.Affine(scale=(0.3,1.3), translate_percent=(-0.3,0.3), rotate=(-10,10), interpolation=2, p=1.0),\n",
        "  A.ToGray(p=0.02),\n",
        "  A.ColorJitter(brightness=(0.8,1.2), contrast=(0.8,1.2), saturation=(0.8,1.2), hue=0.15, p=1.0),\n",
        "  ])\n",
        "\n",
        "val_transform = A.Compose([ #Here only a resize, but val transform could be more complex (center crop, padding, etc)\n",
        "  A.Resize(image_size, image_size, interpolation=2, p=1.0)])\n",
        "\n",
        "\n",
        "#This funtion allow to launch data augmentation on a separate thread.\n",
        "#This way we can train on the GPU and generate new agumented examples in parallel.\n",
        "def data_augm():\n",
        "  input_data, targets = create_augmented_batch(train_transform)\n",
        "  cnn.delete_dataset(\"TRAIN_buf\", silent=1)\n",
        "  cnn.create_dataset(\"TRAIN_buf\", nb_images_per_iter, input_data[:,:], targets[:,:], silent=1)\n",
        "  return\n",
        "\n",
        "#In case the creation of new augmented data is too long compared to training, you can\n",
        "#increase the number of training iteration over a single augmentation\n",
        "nb_iter_per_augm = 2\n",
        "if(nb_iter_per_augm > 1):\n",
        "  shuffle_frequency = 1\n",
        "else:\n",
        "  shuffle_frequency = 0\n",
        "\n",
        "\n",
        "total_iter = 2000 #Should be increased with the complexity of the network and task\n",
        "load_iter = 1000 #Used to reload a model at a given iteration\n",
        "\n",
        "if (len(sys.argv) > 1):\n",
        "  load_iter = int(sys.argv[1])\n",
        "\n",
        "start_iter = int(load_iter / nb_iter_per_augm)\n",
        "\n",
        "cnn.init(in_dim=i_ar([image_size,image_size]), in_nb_ch=3, out_dim=2,\n",
        "  bias=0.1, b_size=16, comp_meth='C_CUDA', dynamic_load=1,\n",
        "  mixed_precision=\"FP32C_FP32A\", adv_size=30)\n",
        "\n",
        "data_prep(nb_images_per_iter, raw_image_size, image_size)\n",
        "\n",
        "input_val, targets_val = create_validation_set(val_transform)\n",
        "cnn.create_dataset(\"VALID\", 2048, input_val[:,:], targets_val[:,:])\n",
        "cnn.create_dataset(\"TEST\", 2048, input_val[:,:], targets_val[:,:])\n",
        "del (input_val, targets_val) #The python arrays are no longer required after import in CIANNA\n",
        "gc.collect()\n",
        "\n",
        "#Create fist augmentation before parallelization\n",
        "input_data, targets = create_augmented_batch(train_transform)\n",
        "cnn.create_dataset(\"TRAIN\", nb_images_per_iter, input_data[:,:], targets[:,:])\n",
        "\n",
        "if(load_iter > 0):\n",
        "  cnn.load(\"net_save/net0_s%04d.dat\"%load_iter, load_iter, bin=1)\n",
        "else:\n",
        "  #Network backbone architecture\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=16  , padding=i_ar([1,1]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "  cnn.norm(group_size=2, activation=\"RELU\")\n",
        "\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=32  , padding=i_ar([1,1]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "  cnn.norm(group_size=2, activation=\"RELU\")\n",
        "\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=64\t, padding=i_ar([1,1]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "  cnn.norm(group_size=4, activation=\"RELU\")\n",
        "\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=128 , padding=i_ar([1,1]), activation=\"RELU\")\n",
        "  cnn.conv(f_size=i_ar([1,1]), nb_filters=64  , padding=i_ar([0,0]), activation=\"RELU\")\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=128 , padding=i_ar([1,1]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "  cnn.norm(group_size=4, activation=\"RELU\")\n",
        "\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=192 , padding=i_ar([1,1]), activation=\"RELU\")\n",
        "  cnn.conv(f_size=i_ar([1,1]), nb_filters=96  , padding=i_ar([0,0]), activation=\"RELU\")\n",
        "  cnn.conv(f_size=i_ar([3,3]), nb_filters=192 , padding=i_ar([1,1]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([2,2]), p_type=\"MAX\")\n",
        "  cnn.norm(group_size=8, activation=\"RELU\")\n",
        "\n",
        "  cnn.conv(f_size=i_ar([1,1]), nb_filters=2 , padding=i_ar([0,0]), activation=\"LIN\")\n",
        "  cnn.pool(p_size=i_ar([1,1]), p_type=\"AVG\", p_global=1, activation=\"SMAX\")\n",
        "\n",
        "\n",
        "cnn.print_arch_tex(\"./arch/\", \"arch\", activation=1, dropout=1)\n",
        "\n",
        "for run_iter in range(start_iter,int(total_iter/nb_iter_per_augm)):\n",
        "\n",
        "  t = Thread(target=data_augm)\n",
        "  t.start()\n",
        "\n",
        "  cnn.train(nb_iter=nb_iter_per_augm, learning_rate=0.002, end_learning_rate=0.00003, shuffle_every=shuffle_frequency ,\\\n",
        "        control_interv=20, confmat=1, momentum=0.9, lr_decay=0.0012, weight_decay=0.001, save_every=20,\\\n",
        "        silent=0, save_bin=1, TC_scale_factor=256.0)\n",
        "\n",
        "  if(run_iter == start_iter):\n",
        "    cnn.perf_eval()\n",
        "\n",
        "  t.join()\n",
        "  cnn.swap_data_buffers(\"TRAIN\")\n",
        "\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "MvRhvumeRWZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate your model\n",
        "\n",
        "The following cell evaluates the accuracy and inference time over the test set.\n",
        "\n",
        "Colab usually puts the GPU into sleep mode after idling for a few seconds. Always run this cell a few times in a row to get the real execution time.\n",
        "\n",
        "Edit the Google Sheet to add your result:  \n",
        "https://docs.google.com/spreadsheets/d/1l3l5zfHfAsi5AF4IU4P-5OQs0nl4cRzQpoHdWkRBqUc/edit?usp=sharing"
      ],
      "metadata": {
        "id": "quIkFmK1TLUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "from helper import *\n",
        "\n",
        "#Comment to access system wide install\n",
        "sys.path.insert(0,glob.glob('/content/CIANNA/src/build/lib.*/')[-1])\n",
        "import CIANNA as cnn\n",
        "\n",
        "def i_ar(int_list):\n",
        "  return np.array(int_list, dtype=\"int\")\n",
        "\n",
        "raw_image_size = 128\n",
        "image_size = 128\n",
        "\n",
        "val_transform = A.Compose([ #Here only a resize, but val transform could be more complex (center crop, padding, etc)\n",
        "  A.Resize(image_size, image_size, interpolation=2, p=1.0)])\n",
        "\n",
        "cnn.init(in_dim=i_ar([image_size,image_size]), in_nb_ch=3, out_dim=2,\n",
        "  bias=0.1, b_size=1024, comp_meth='C_CUDA', dynamic_load=1,\n",
        "  mixed_precision=\"FP16C_FP32A\", adv_size=30, inference_only=1)\n",
        "\n",
        "data_prep(0, raw_image_size, image_size, test_mode=1)\n",
        "\n",
        "#Compute on only half the validation set to reduce memory footprint\n",
        "input_test, targets_test = create_validation_set(val_transform)\n",
        "cnn.create_dataset(\"TEST\", 2048, input_test[:,:], targets_test[:,:])\n",
        "\n",
        "#Change epochs to load based on your own training\n",
        "load_epoch = 1000\n",
        "if(load_epoch == 0):\n",
        "  #You can also download a pretrained model with the same architecture\n",
        "  os.system(\"wget https://share.obspm.fr/s/dyrJikKH2A7Sxso/download/arch2_res128_err1.76_ms89.dat\")\n",
        "  cnn.load(\"arch2_res128_err1.76_ms89.dat\", load_epoch, bin=1)\n",
        "else:\n",
        "  cnn.load(\"net_save/net0_s%04d.dat\"%load_epoch, load_epoch, bin=1)\n",
        "\n",
        "\n",
        "\n",
        "#Run forward a first time to wake up the GPU and save the result\n",
        "cnn.forward(repeat=1, no_error=1, saving=2, drop_mode=\"AVG_MODEL\")\n",
        "\n",
        "start = time.perf_counter()\n",
        "#Run forward to evaluate raw model performance\n",
        "cnn.forward(no_error=1, saving=0, drop_mode=\"AVG_MODEL\")\n",
        "end = time.perf_counter()\n",
        "\n",
        "cnn.perf_eval()\n",
        "compute_time = (end-start)*1000 #in miliseconds\n",
        "print (\"Inference time: %f ms (%d ips)\"%(compute_time, int(2048/compute_time)))\n",
        "\n",
        "\n",
        "pred_results = np.fromfile(\"fwd_res/net0_%04d.dat\"%(load_epoch), dtype=\"float32\")\n",
        "pred_result = np.reshape(pred_results, (2048,-1))\n",
        "\n",
        "pred_correct = np.shape(np.where(np.argmax(pred_result[:,:2], axis=1) == np.argmax(targets_test[:,:], axis=1)))[1]\n",
        "pred_accuracy = (pred_correct/2048)*100.0\n",
        "print (\"Accuracy: %f, Error rate: %f\"%(pred_accuracy, 100.0-pred_accuracy))\n",
        "\n",
        "EOF"
      ],
      "metadata": {
        "id": "oMM45xmgTgFj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}